{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'Beef Roll'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 50\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;66;03m# Train the Central Model using RandomForestRegressor\u001b[39;00m\n\u001b[1;32m     49\u001b[0m central_model_rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m, n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n\u001b[0;32m---> 50\u001b[0m \u001b[43mcentral_model_rf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m# Evaluate the Central Model\u001b[39;00m\n\u001b[1;32m     53\u001b[0m y_pred_rf \u001b[38;5;241m=\u001b[39m central_model_rf\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/ensemble/_forest.py:363\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    360\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m issparse(y):\n\u001b[1;32m    361\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msparse multilabel-indicator for y is not supported.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 363\u001b[0m X, y \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    364\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    365\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    366\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    367\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsc\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    368\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDTYPE\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    369\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    370\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    371\u001b[0m \u001b[38;5;66;03m# _compute_missing_values_in_feature_mask checks if X has missing values and\u001b[39;00m\n\u001b[1;32m    372\u001b[0m \u001b[38;5;66;03m# will raise an error if the underlying tree base estimator can't handle missing\u001b[39;00m\n\u001b[1;32m    373\u001b[0m \u001b[38;5;66;03m# values. Only the criterion is required to determine if the tree supports\u001b[39;00m\n\u001b[1;32m    374\u001b[0m \u001b[38;5;66;03m# missing values.\u001b[39;00m\n\u001b[1;32m    375\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)(criterion\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcriterion)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/base.py:650\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, cast_to_ndarray, **check_params)\u001b[0m\n\u001b[1;32m    648\u001b[0m         y \u001b[38;5;241m=\u001b[39m check_array(y, input_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124my\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_y_params)\n\u001b[1;32m    649\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 650\u001b[0m         X, y \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    651\u001b[0m     out \u001b[38;5;241m=\u001b[39m X, y\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mensure_2d\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:1301\u001b[0m, in \u001b[0;36mcheck_X_y\u001b[0;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[0m\n\u001b[1;32m   1296\u001b[0m         estimator_name \u001b[38;5;241m=\u001b[39m _check_estimator_name(estimator)\n\u001b[1;32m   1297\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m   1298\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires y to be passed, but the target y is None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1299\u001b[0m     )\n\u001b[0;32m-> 1301\u001b[0m X \u001b[38;5;241m=\u001b[39m \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1302\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1303\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1304\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1305\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1306\u001b[0m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1307\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1308\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1309\u001b[0m \u001b[43m    \u001b[49m\u001b[43mforce_all_finite\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mforce_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1310\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1312\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1313\u001b[0m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1314\u001b[0m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1315\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mX\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1316\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1318\u001b[0m y \u001b[38;5;241m=\u001b[39m _check_y(y, multi_output\u001b[38;5;241m=\u001b[39mmulti_output, y_numeric\u001b[38;5;241m=\u001b[39my_numeric, estimator\u001b[38;5;241m=\u001b[39mestimator)\n\u001b[1;32m   1320\u001b[0m check_consistent_length(X, y)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/sklearn/utils/validation.py:929\u001b[0m, in \u001b[0;36mcheck_array\u001b[0;34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m pandas_requires_conversion:\n\u001b[1;32m    925\u001b[0m     \u001b[38;5;66;03m# pandas dataframe requires conversion earlier to handle extension dtypes with\u001b[39;00m\n\u001b[1;32m    926\u001b[0m     \u001b[38;5;66;03m# nans\u001b[39;00m\n\u001b[1;32m    927\u001b[0m     \u001b[38;5;66;03m# Use the original dtype for conversion if dtype is None\u001b[39;00m\n\u001b[1;32m    928\u001b[0m     new_dtype \u001b[38;5;241m=\u001b[39m dtype_orig \u001b[38;5;28;01mif\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m dtype\n\u001b[0;32m--> 929\u001b[0m     array \u001b[38;5;241m=\u001b[39m \u001b[43marray\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_dtype\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    930\u001b[0m     \u001b[38;5;66;03m# Since we converted here, we do not need to convert again later\u001b[39;00m\n\u001b[1;32m    931\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/generic.py:6643\u001b[0m, in \u001b[0;36mNDFrame.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m   6637\u001b[0m     results \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m   6638\u001b[0m         ser\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy, errors\u001b[38;5;241m=\u001b[39merrors) \u001b[38;5;28;01mfor\u001b[39;00m _, ser \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m   6639\u001b[0m     ]\n\u001b[1;32m   6641\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6642\u001b[0m     \u001b[38;5;66;03m# else, only a single dtype is given\u001b[39;00m\n\u001b[0;32m-> 6643\u001b[0m     new_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_mgr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6644\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_from_mgr(new_data, axes\u001b[38;5;241m=\u001b[39mnew_data\u001b[38;5;241m.\u001b[39maxes)\n\u001b[1;32m   6645\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m res\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m, method\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mastype\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/managers.py:430\u001b[0m, in \u001b[0;36mBaseBlockManager.astype\u001b[0;34m(self, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    428\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 430\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    431\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mastype\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    432\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    433\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    434\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    435\u001b[0m \u001b[43m    \u001b[49m\u001b[43musing_cow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43musing_copy_on_write\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    436\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/managers.py:363\u001b[0m, in \u001b[0;36mBaseBlockManager.apply\u001b[0;34m(self, f, align_keys, **kwargs)\u001b[0m\n\u001b[1;32m    361\u001b[0m         applied \u001b[38;5;241m=\u001b[39m b\u001b[38;5;241m.\u001b[39mapply(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    362\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 363\u001b[0m         applied \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    364\u001b[0m     result_blocks \u001b[38;5;241m=\u001b[39m extend_blocks(applied, result_blocks)\n\u001b[1;32m    366\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mfrom_blocks(result_blocks, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes)\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/internals/blocks.py:758\u001b[0m, in \u001b[0;36mBlock.astype\u001b[0;34m(self, dtype, copy, errors, using_cow, squeeze)\u001b[0m\n\u001b[1;32m    755\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan not squeeze with more than one column.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    756\u001b[0m     values \u001b[38;5;241m=\u001b[39m values[\u001b[38;5;241m0\u001b[39m, :]  \u001b[38;5;66;03m# type: ignore[call-overload]\u001b[39;00m\n\u001b[0;32m--> 758\u001b[0m new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array_safe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    760\u001b[0m new_values \u001b[38;5;241m=\u001b[39m maybe_coerce_values(new_values)\n\u001b[1;32m    762\u001b[0m refs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/dtypes/astype.py:237\u001b[0m, in \u001b[0;36mastype_array_safe\u001b[0;34m(values, dtype, copy, errors)\u001b[0m\n\u001b[1;32m    234\u001b[0m     dtype \u001b[38;5;241m=\u001b[39m dtype\u001b[38;5;241m.\u001b[39mnumpy_dtype\n\u001b[1;32m    236\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 237\u001b[0m     new_values \u001b[38;5;241m=\u001b[39m \u001b[43mastype_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mTypeError\u001b[39;00m):\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;66;03m# e.g. _astype_nansafe can fail on object-dtype of strings\u001b[39;00m\n\u001b[1;32m    240\u001b[0m     \u001b[38;5;66;03m#  trying to convert to float\u001b[39;00m\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/dtypes/astype.py:182\u001b[0m, in \u001b[0;36mastype_array\u001b[0;34m(values, dtype, copy)\u001b[0m\n\u001b[1;32m    179\u001b[0m     values \u001b[38;5;241m=\u001b[39m values\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n\u001b[1;32m    181\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 182\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[43m_astype_nansafe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    184\u001b[0m \u001b[38;5;66;03m# in pandas we don't store numpy str dtypes, so convert to object\u001b[39;00m\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(dtype, np\u001b[38;5;241m.\u001b[39mdtype) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(values\u001b[38;5;241m.\u001b[39mdtype\u001b[38;5;241m.\u001b[39mtype, \u001b[38;5;28mstr\u001b[39m):\n",
      "File \u001b[0;32m~/Library/Python/3.9/lib/python/site-packages/pandas/core/dtypes/astype.py:133\u001b[0m, in \u001b[0;36m_astype_nansafe\u001b[0;34m(arr, dtype, copy, skipna)\u001b[0m\n\u001b[1;32m    129\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[1;32m    131\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m copy \u001b[38;5;129;01mor\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mobject\u001b[39m:\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;66;03m# Explicit copy, or required since NumPy can't view from / to object.\u001b[39;00m\n\u001b[0;32m--> 133\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43marr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mastype(dtype, copy\u001b[38;5;241m=\u001b[39mcopy)\n",
      "\u001b[0;31mValueError\u001b[0m: could not convert string to float: 'Beef Roll'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib  # For saving and loading the model\n",
    "\n",
    "# Function to preprocess the data\n",
    "def preprocess_data(df):\n",
    "    # Convert Date to datetime if not already\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    \n",
    "    # Extract temporal features\n",
    "    df[\"Month\"] = df[\"Date\"].dt.month\n",
    "    df[\"Day\"] = df[\"Date\"].dt.day\n",
    "    df[\"Weekday\"] = df[\"Date\"].dt.weekday\n",
    "    df[\"Is_Weekend\"] = df[\"Weekday\"].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    df = pd.get_dummies(df, columns=[\"Weather\", \"Event\", \"Restaurant Type\", \"Restaurant Address\"], drop_first=True)\n",
    "    \n",
    "    # Drop unused columns\n",
    "    df = df.drop(columns=[\"Date\", \"ID\", \"Restaurant ID\", 'Restaurant Annual Sale'])\n",
    "    return df\n",
    "\n",
    "# Load data from local files\n",
    "def load_data(file_paths):\n",
    "    \"\"\"\n",
    "    Load and combine data from a list of file paths.\n",
    "    \"\"\"\n",
    "    dataframes = [pd.read_excel(file) for file in file_paths]\n",
    "    return pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# File paths for the existing restaurant datasets\n",
    "file_paths = [\"test_data/Central.xlsx\"]\n",
    "\n",
    "# Load and preprocess the data\n",
    "central_data = load_data(file_paths)\n",
    "processed_data = preprocess_data(central_data)\n",
    "\n",
    "# Define features and target\n",
    "X = processed_data.drop(columns=[\"Number Sold\"])\n",
    "y = processed_data[\"Number Sold\"]\n",
    "\n",
    "# Split data into training and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42, shuffle=False)\n",
    "\n",
    "# Train the Central Model using RandomForestRegressor\n",
    "central_model_rf = RandomForestRegressor(random_state=42, n_estimators=100, max_depth=10)\n",
    "central_model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the Central Model\n",
    "y_pred_rf = central_model_rf.predict(X_val)\n",
    "mae_rf = mean_absolute_error(y_val, y_pred_rf)\n",
    "rmse_rf = mean_squared_error(y_val, y_pred_rf, squared=False)\n",
    "\n",
    "# Save the trained model to a file\n",
    "model_path = \"central_model_rf.pkl\"\n",
    "joblib.dump(central_model_rf, model_path)\n",
    "\n",
    "print(f\"Model saved to {model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Discount', 'Price', 'Restaurant Annual Sale', 'Month', 'Day',\n",
       "       'Weekday', 'Is_Weekend', 'Weather_Rainy', 'Weather_Sunny',\n",
       "       'Weather_Windy', 'Event_Chinese New Year',\n",
       "       'Event_Dragon Boat Festival', 'Event_Lantern Festival',\n",
       "       'Event_Lunar Ghost Festival', \"Event_Valentine's Day\",\n",
       "       'Restaurant Type_Bakery', 'Restaurant Type_Bar',\n",
       "       'Restaurant Type_Bubble Tea Shop', 'Restaurant Type_Buffet',\n",
       "       'Restaurant Type_Caf茅', 'Restaurant Type_Cantonese Fine Dining',\n",
       "       'Restaurant Type_Casual Dining', 'Restaurant Type_Cha Chaan Teng',\n",
       "       'Restaurant Type_Congee and Noodle Eatery',\n",
       "       'Restaurant Type_Dim Sum Restaurant', 'Restaurant Type_Fast Food',\n",
       "       'Restaurant Type_Fine Dining', 'Restaurant Type_Food Truck',\n",
       "       'Restaurant Type_Hot Pot Restaurant',\n",
       "       'Restaurant Type_Noodle Shop',\n",
       "       'Restaurant Type_Seafood Restaurant',\n",
       "       'Restaurant Type_Seafood Stall', 'Restaurant Type_Steakhouse',\n",
       "       'Restaurant Type_Street Food Vendor'], dtype=object)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import joblib  # For loading the model\n",
    "\n",
    "# Function to preprocess the test data (same as training data)\n",
    "def preprocess_data(df):\n",
    "    # Convert Date to datetime if not already\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    \n",
    "    # Extract temporal features\n",
    "    df[\"Month\"] = df[\"Date\"].dt.month\n",
    "    df[\"Day\"] = df[\"Date\"].dt.day\n",
    "    df[\"Weekday\"] = df[\"Date\"].dt.weekday\n",
    "    df[\"Is_Weekend\"] = df[\"Weekday\"].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    df = pd.get_dummies(df, columns=[\"Weather\", \"Event\", \"Restaurant Type\"], drop_first=True)\n",
    "    \n",
    "    # Align columns with the training data\n",
    "    # Fill missing columns with 0 if they are not present in the test data\n",
    "    missing_cols = [col for col in X_train.columns if col not in df.columns]\n",
    "    for col in missing_cols:\n",
    "        df[col] = 0\n",
    "    \n",
    "    # Ensure the columns match the training data\n",
    "    df = df[X_train.columns]\n",
    "    return df\n",
    "\n",
    "# Load the saved model\n",
    "model_path = \"Model/central_model_rf.pkl\"\n",
    "central_model = joblib.load(model_path)\n",
    "central_model.feature_names_in_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Date    ID    Name  Number Sold  Discount  Price Event Weather  \\\n",
      "0   2024-06-01  P008  Dish 1          313      0.00     25   NaN   Sunny   \n",
      "1   2024-06-01  P008  Dish 2          362      0.00     24   NaN   Sunny   \n",
      "2   2024-06-01  P008  Dish 3          308      0.00     15   NaN   Sunny   \n",
      "3   2024-06-01  P008  Dish 4          320      0.00     27   NaN   Sunny   \n",
      "4   2024-06-02  P009  Dish 1          674      0.20     16   NaN  Cloudy   \n",
      "..         ...   ...     ...          ...       ...    ...   ...     ...   \n",
      "607 2024-10-30  P041  Dish 4          486      0.25     27   NaN   Sunny   \n",
      "608 2024-10-31  P042  Dish 1          276      0.00     22   NaN   Rainy   \n",
      "609 2024-10-31  P042  Dish 2          334      0.00     21   NaN   Rainy   \n",
      "610 2024-10-31  P042  Dish 3          242      0.00     14   NaN   Rainy   \n",
      "611 2024-10-31  P042  Dish 4          234      0.00     21   NaN   Rainy   \n",
      "\n",
      "     Restaurant Type Restaurant ID Restaurant Address  Restaurant Annual Sale  \n",
      "0    Bubble Tea Shop          R121           Wan Chai                   94618  \n",
      "1    Bubble Tea Shop          R121           Wan Chai                   94618  \n",
      "2    Bubble Tea Shop          R121           Wan Chai                   94618  \n",
      "3    Bubble Tea Shop          R121           Wan Chai                   94618  \n",
      "4    Bubble Tea Shop          R121           Wan Chai                   94618  \n",
      "..               ...           ...                ...                     ...  \n",
      "607  Bubble Tea Shop          R121           Wan Chai                   94618  \n",
      "608  Bubble Tea Shop          R121           Wan Chai                   94618  \n",
      "609  Bubble Tea Shop          R121           Wan Chai                   94618  \n",
      "610  Bubble Tea Shop          R121           Wan Chai                   94618  \n",
      "611  Bubble Tea Shop          R121           Wan Chai                   94618  \n",
      "\n",
      "[612 rows x 12 columns]\n",
      "New User Model saved at: new_user_model_rf.pkl\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import joblib\n",
    "\n",
    "# Load the saved central model\n",
    "model_path = \"Model/central_model_rf.pkl\"  # Replace with your central model path\n",
    "try:\n",
    "    central_model = joblib.load(model_path)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"Central model file not found. Ensure the file exists at the specified path.\")\n",
    "\n",
    "# Load new user registration data\n",
    "new_user_data_path = \"test_data/R121_signup.xlsx\"  # Replace with your new user data path\n",
    "try:\n",
    "    new_user_data = pd.read_excel(new_user_data_path)\n",
    "    print(new_user_data)\n",
    "except FileNotFoundError:\n",
    "    raise FileNotFoundError(\"New user signup data not found. Ensure the file exists at the specified path.\")\n",
    "\n",
    "# Preprocess the new user registration data\n",
    "def preprocess_data(df, reference_columns):\n",
    "    \"\"\"\n",
    "    Preprocesses data and ensures alignment with reference_columns.\n",
    "    \"\"\"\n",
    "    # Convert Date to datetime if not already\n",
    "    df[\"Date\"] = pd.to_datetime(df[\"Date\"])\n",
    "    y = df['Number Sold']\n",
    "    # Extract temporal features\n",
    "    df[\"Month\"] = df[\"Date\"].dt.month\n",
    "    df[\"Day\"] = df[\"Date\"].dt.day\n",
    "    df[\"Weekday\"] = df[\"Date\"].dt.weekday\n",
    "    df[\"Is_Weekend\"] = df[\"Weekday\"].isin([5, 6]).astype(int)\n",
    "    \n",
    "    # One-hot encode categorical variables\n",
    "    df = pd.get_dummies(df, columns=[\"Weather\", \"Event\", \"Restaurant Type\"], drop_first=True)\n",
    "    \n",
    "    # Align columns with the central model's features\n",
    "    missing_cols = [col for col in reference_columns if col not in df.columns]\n",
    "    for col in missing_cols:\n",
    "        df[col] = 0\n",
    "    df = df[reference_columns]\n",
    "    df[\"Number Sold\"] = y\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Preprocess the new user data\n",
    "processed_new_user_data = preprocess_data(new_user_data, central_model.feature_names_in_)\n",
    "#print(processed_new_user_data.columns)\n",
    "# Define features and target for the new user data\n",
    "X_new_user = processed_new_user_data.drop(columns=[\"Number Sold\"])\n",
    "y_new_user = processed_new_user_data[\"Number Sold\"]\n",
    "\n",
    "# Train a new model initialized from the central model\n",
    "new_user_model = RandomForestRegressor(\n",
    "    random_state=42,\n",
    "    n_estimators=central_model.n_estimators,\n",
    "    max_depth=central_model.max_depth,\n",
    ")\n",
    "new_user_model.fit(X_new_user, y_new_user)\n",
    "\n",
    "# Save the new user-specific model\n",
    "new_user_model_path = \"new_user_model_rf.pkl\" \n",
    "joblib.dump(new_user_model, new_user_model_path)\n",
    "print(f\"New User Model saved at: {new_user_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "数据已成功导入到 Neo4j！\n"
     ]
    }
   ],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "import pandas as pd\n",
    "\n",
    "# 连接到Neo4j数据库\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"88888888\"))  # 修改为你的Neo4j地址和密码\n",
    "\n",
    "# 加载数据\n",
    "file_path = 'test_data/R121_daily.xlsx'\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# 清理数据库以防止冲突\n",
    "#graph.delete_all()\n",
    "\n",
    "# 遍历数据并更新/插入节点和关系\n",
    "for _, row in df.iterrows():\n",
    "    # 创建或更新 Restaurant 节点\n",
    "    restaurant = Node(\n",
    "        \"Restaurant\",\n",
    "        id=row[\"Restaurant ID\"],\n",
    "        type=row[\"Restaurant Type\"],\n",
    "        address=row[\"Restaurant Address\"],\n",
    "        annual_sale=row[\"Restaurant Annual Sale\"],\n",
    "    )\n",
    "    graph.merge(restaurant, \"Restaurant\", \"id\")\n",
    "\n",
    "    # 创建或更新 Dish 节点\n",
    "    dish = Node(\n",
    "        \"Dish\",\n",
    "        id=row[\"ID\"],\n",
    "        name=row[\"Name\"],\n",
    "        price=row[\"Price\"],\n",
    "        discount=row[\"Discount\"],\n",
    "    )\n",
    "    graph.merge(dish, \"Dish\", \"id\")\n",
    "\n",
    "    # 创建或更新 Date 节点\n",
    "    date = Node(\"Date\", date=row[\"Date\"])\n",
    "    graph.merge(date, \"Date\", \"date\")\n",
    "\n",
    "    # 创建或更新 Weather 节点\n",
    "    weather = Node(\"Weather\", condition=row[\"Weather\"])\n",
    "    graph.merge(weather, \"Weather\", \"condition\")\n",
    "\n",
    "    # 创建或更新 Event 节点（如果有事件）\n",
    "    if pd.notna(row[\"Event\"]):\n",
    "        event = Node(\"Event\", name=row[\"Event\"])\n",
    "        graph.merge(event, \"Event\", \"name\")\n",
    "    else:\n",
    "        event = None\n",
    "\n",
    "    # 创建或更新关系: Restaurant -> Dish\n",
    "    rel_sold = Relationship(restaurant, \"SOLD\", dish, number_sold=row[\"Number Sold\"])\n",
    "    graph.merge(rel_sold)\n",
    "\n",
    "    # 创建或更新关系: Dish -> Date\n",
    "    rel_date = Relationship(dish, \"SOLD_ON\", date)\n",
    "    graph.merge(rel_date)\n",
    "\n",
    "    # 创建或更新关系: Date -> Weather\n",
    "    rel_weather = Relationship(date, \"WEATHER\", weather)\n",
    "    graph.merge(rel_weather)\n",
    "\n",
    "    # 创建或更新关系: Date -> Event（如果有事件）\n",
    "    if event:\n",
    "        rel_event = Relationship(date, \"EVENT\", event)\n",
    "        graph.merge(rel_event)\n",
    "\n",
    "print(\"数据已成功更新到 Neo4j！\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "import pandas as pd\n",
    "graph = Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"88888888\"))  # 修改为你的Neo4j地址和密码\n",
    "a = Node(\"Bro\",add = 1,aaa = 2 )\n",
    "b = Node('Bro',add = 1,aaa = 2,acc = 3)\n",
    "\n",
    "graph.merge(b, 'Bro', 'add')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from py2neo import Graph, Node, Relationship\n",
    "\n",
    "def read_files(path):\n",
    "        db =  Graph(\"bolt://localhost:7687\", auth=(\"neo4j\", \"88888888\"))\n",
    "        df = pd.read_excel(path)\n",
    "        for _,row in df.iterrows():\n",
    "            \n",
    "            restaurant = Node(\"Restaurant\",\n",
    "                            ID = str(row[\"Restaurant ID\"]),\n",
    "                            type = str(row[\"Restaurant Type\"]),\n",
    "                            Address = str(row[\"Restaurant Address\"]))\n",
    "            location = Node(\"Location\",location = str(row[\"Restaurant Address\"]))\n",
    "            dish = Node(\"Dish\",name = str(row[\"Name\"]),ID = str(row[\"ID\"]))\n",
    "            record = Node(\"Record\",time = row['Date'],number = int(row[\"Number Sold\"]),\n",
    "                        discount = float(row[\"Discount\"]),price = float(row[\"Price\"]))\n",
    "            weather = Node(\"Weather\",weather = str(row['Weather']))\n",
    "            Event =  Node(\"Event\",event = str(row['Event']))\n",
    "\n",
    "            db.merge(restaurant,\"Restaurant\",\"ID\")\n",
    "            db.merge(location,\"Location\",\"location\")\n",
    "            db.merge(dish,\"Dish\",\"ID\")\n",
    "            db.merge(record,\"Record\",\"time\")\n",
    "            db.merge(weather,\"Weather\",\"weather\")\n",
    "            db.merge(Event,\"Event\",\"event\")\n",
    "\n",
    "            db.merge(Relationship(restaurant,\"located at\",location),\"located at\",\"location\")\n",
    "            db.merge(Relationship(restaurant,\"sells\",dish),)\n",
    "            db.merge(Relationship(dish,\"sold\",record))\n",
    "            db.merge(Relationship(record,\"at\",location))\n",
    "            db.merge(Relationship(record,\"with\",weather))\n",
    "            db.merge(Relationship(record,\"during\",Event))\n",
    "\n",
    "read_files(\"test_data/Central.xlsx\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
